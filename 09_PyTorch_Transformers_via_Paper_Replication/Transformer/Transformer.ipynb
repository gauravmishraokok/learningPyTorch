{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RTQAO_sXrUO"
      },
      "source": [
        "# **Implementing Transformer by Replicating** ***Attention Is All You Need***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Transformer for Machine Translation**\n",
        "\n",
        "This project implements a Transformer model for machine translation, based on the architecture described in the paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The Transformer model revolutionized the field of natural language processing by introducing a novel architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolutions.\n",
        "\n",
        "### **Project Structure**\n",
        "\n",
        "The implementation is divided into several modules, each handling a specific component of the Transformer architecture:\n",
        "\n",
        "1. `_01_inputEmbeddings.py`: Implements the input embedding layer that converts token IDs into dense vectors.\n",
        "\n",
        "2. `_02_positionalEncoding.py`: Adds positional information to the input embeddings, allowing the model to understand token order.\n",
        "\n",
        "3. `_03_layerNormalisation.py`: Implements layer normalization, which helps stabilize the learning process.\n",
        "\n",
        "4. `_04_feedForwardNetwork.py`: Contains the feed-forward network used in both encoder and decoder layers.\n",
        "\n",
        "5. `_05_multiHeadAttention.py`: Implements the multi-head attention mechanism, the core component of the Transformer.\n",
        "\n",
        "6. `_06_residualConnection.py`: Handles residual connections, which help in training deep networks.\n",
        "\n",
        "7. `_07_encoder.py`: Builds the encoder stack, consisting of multiple encoder layers.\n",
        "\n",
        "8. `_08_decoder.py`: Constructs the decoder stack, with multiple decoder layers.\n",
        "\n",
        "9. `_09_projectionLayer.py`: Implements the final projection layer that converts decoder output to vocabulary probabilities.\n",
        "\n",
        "10. `_10_transformer.py`: Assembles all components into the complete Transformer model.\n",
        "\n",
        "11. `_11_buildTransformer.py`: Contains function to build the Transformer model.\n",
        "\n",
        "12. `_12_bilingualDataset.py`: Handles data preprocessing and batching for bilingual translation tasks.\n",
        "\n",
        "13. `_13_buildTokenizer_DataLoader_and_Transformer.py`: Sets up the tokenizers, data loaders, and initializes the Transformer model.\n",
        "\n",
        "14. `_14_config.py`: Manages configuration settings for the model and training process.\n",
        "\n",
        "15. `_15_train.py`: Implements the training loop and validation process.\n",
        "\n",
        "### **Translation Process**\n",
        "\n",
        "The Transformer model processes source sentences and generates target translations through the following steps:\n",
        "\n",
        "1. **Tokenization**: Source sentences are tokenized and converted to token IDs.\n",
        "2. **Embedding**: Token IDs are transformed into dense vector representations.\n",
        "3. **Positional Encoding**: Position information is added to the embeddings.\n",
        "4. **Encoding**: The encoder processes the source sentence, creating a context-rich representation.\n",
        "5. **Decoding**: The decoder generates the target translation, attending to both the encoded source and previously generated tokens.\n",
        "6. **Output Projection**: The decoder's output is projected onto the target vocabulary to produce token probabilities.\n",
        "7. **Generation**: Target tokens are generated either greedily or using beam search.\n",
        "\n",
        "This implementation showcases the power of the Transformer architecture in handling sequence-to-sequence tasks like machine translation, demonstrating its ability to capture long-range dependencies and parallelize computation effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txKAlib1XrUP"
      },
      "source": [
        "## **I. Components of the Transformer ->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNhkpZ49XrUP"
      },
      "source": [
        "### 1. Input Embeddings ->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ly0DV1-SXrUP"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_01_inputEmbeddings.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model:int, vocab_size:int ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  #Dimensionality -> d_model = 512: You choose to represent each word by a 512-dimensional vector.\n",
        "        self.vocab_size = vocab_size #Number of Tokens\n",
        "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.embedding(x)*math.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeaRhILkXrUQ"
      },
      "source": [
        "### 2. Positional Encoding ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSsq-QlfXrUQ"
      },
      "source": [
        "\\begin{align}\n",
        "PE(pos, 2i)   &= \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}} \\right) \\\\\n",
        "PE(pos, 2i+1) &= \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}} \\right)\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rAo_5cpkXrUQ"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_02_positionalEncoding.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model:int, seq_len:int, dropout:float ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = dropout\n",
        "\n",
        "        #Creating a Matrix of shape (seq_len , d_model)\n",
        "        pe = torch.zeros(seq_len,d_model)\n",
        "\n",
        "        #Creating a position vector of length seq_len\n",
        "        position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1) #[0,1,2,...,n]\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0,d_model,2)).float() * (-math.log(10000)/d_model) #This comes from the denominator of the function.\n",
        "\n",
        "\n",
        "        #Applying sin to even positions and cos to odd positions\n",
        "        pe[:, 0::2] = torch.sin(position*div_term)\n",
        "        pe[:, 1::2] = torch.cos(position*div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(dim = 0) # [1,seq_len,d_model]\n",
        "\n",
        "        #Register Buffer\n",
        "             #--> Is used for saving positonal encoding to model's state_dict as it is not updated during any backward propagation step but is needed for reliability and reusability.\n",
        "        self.register_buffer('pe',pe)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        #Input tokens  --> Input tokens  + Position of the respective tokens\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "\n",
        "        #Dropout layer\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrytcG6xXrUR"
      },
      "source": [
        "### 3. Layer Normalisation ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK0AKadEXrUR"
      },
      "source": [
        "\n",
        "\\begin{align}\n",
        "\\large \\hat{x}_j=\\frac{x_j-\\mu_j}{\\sqrt{\\sigma_j^2+\\epsilon}}\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UXNrpgmcXrUR"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_03_layerNormalisation.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps:float = 10**-6):\n",
        "        super().__init__()\n",
        "\n",
        "        #Epsilon is a small value added for numerical stability and also to avoid division by 0\n",
        "        self.eps = eps\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(1))    #This is multiplied\n",
        "        self.bias = nn.Parameter(torch.zeros(1))    #This is added\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        mean = x.mean(dim = -1 , keepdim=True) #Usually mean doesnt keep dimension.\n",
        "        std = x.std(dim = -1 , keepdim=True) #Usually std doesnt keep dimension.\n",
        "\n",
        "        return self.alpha * (x-mean)/torch.sqrt(std + self.eps)   + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACk-GTfwXrUR"
      },
      "source": [
        "### 4. Feed-Forward Network ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HyM5KIOXrUR"
      },
      "source": [
        "The Feed Forward Network (FFN) in the Transformer model is represented as:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\large \\operatorname{FFN}(x)=\\max \\left(0, x W_1+b_1\\right) W_2+b_2\n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1yKnE8-PXrUR"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_04_feedForwardNetwork.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model: int , d_ff:int , dropout:float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1=nn.Linear(in_features=d_model,out_features=d_ff) #W1,B1\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.linear_2 = nn.Linear(d_ff,d_model) #W2,B2\n",
        "\n",
        "    def forward(self, x):\n",
        "        #(Batch_len , Seq_len , d_model) -> (Batch_len , Seq_len , d_ff) ->(Batch_len , Seq_len , d_model)\n",
        "\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVh9FrlHXrUR"
      },
      "source": [
        "### 5. Multi-Head Attention ->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7zGq4uiXrUS"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 90%; /* Adjust the width as needed */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <img src=\"https://i.ibb.co/Y0mbNbH/image.png\" alt=\"Transformer Encoder Block\" class=\"centered-image\">\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nk_aoprJXrUS"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_05_multiHeadAttention.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model:int,h:int,dropout:float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.h=h\n",
        "        assert d_model%h==0 , 'Embedding dimension must be divisible by number of heads'\n",
        "\n",
        "        self.d_k = d_model//h  #d_k = d_model/h\n",
        "\n",
        "        self.w_q = nn.Linear(d_model,d_model) #Wq\n",
        "        self.w_k = nn.Linear(d_model,d_model) #Wk\n",
        "        self.w_v = nn.Linear(d_model,d_model) #Wv\n",
        "\n",
        "        self.w_o = nn.Linear(d_model,d_model) #Wo as d_v is same as d_k and d_k*h = d_model\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query,key,value,mask,dropout:nn.Dropout):\n",
        "        #Calculating the attention score\n",
        "        d_k = query.shape[-1]\n",
        "        attention_scores = (query @ key.transpose(-2,-1))/math.sqrt(d_k) # @ --> Matrix Multiplication\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0 , -1e9) #-1e9 --> -Infinity. These values later become 0 after softmax\n",
        "\n",
        "        attention_scores = attention_scores.softmax(dim = -1) #(Batch , h , Seq_len , Seq_len)\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        return (attention_scores @ value) , attention_scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "\n",
        "        #Getting the Q',K' & V'\n",
        "\n",
        "        query = self.w_q(q) # (Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
        "        key = self.w_k(k) # (Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
        "        value = self.w_v(v) # (Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
        "\n",
        "        #Splitting the d_model dimension into h heads\n",
        "\n",
        "        #Here query.shape[0] -> Batch & query.shape[1] -> Seq_len\n",
        "        #Final shape of query after transpose -> (Batch , h , Seq_len , d_k)\n",
        "        query = query.reshape(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "        key = key.reshape(key.shape[0] , key.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "        value=key.reshape(value.shape[0] , value.shape[1],self.h,self.d_k).transpose(1,2)\n",
        "\n",
        "        x,self.attention_score = MultiHeadAttention.attention(query,key,value,mask,self.dropout)\n",
        "        #Shape of x -> (Batch , h , Seq_len , d_k)\n",
        "        #Shape of attention_score -> (Batch , h , Seq_len , Seq_len)\n",
        "\n",
        "        x= x.transpose(1,2).contiguous().view(x.shape[0], -1 , self.d_k * self.h) #(Batch , Seq_len , d_model)\n",
        "\n",
        "\n",
        "        return self.w_o(x) #(Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6gQCIOaXrUS"
      },
      "source": [
        "### 6. Residual Connection ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olss7XrWXrUS"
      },
      "source": [
        "Used all over the model to add the input x to modified input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HXEGFW8HXrUS"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_06_residualConnection.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from modules._03_layerNormalisation import LayerNormalization\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self,features,dropout):\n",
        "        super(ResidualConnection,self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self,x,sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAoWpvQKXrUS"
      },
      "source": [
        "### 7. Encoder Block ->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmqxRdg3XrUS"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%; /* Adjust the width as needed */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <img src=\"https://raw.githubusercontent.com/hyunwoongko/transformer/master/image/model.png\" alt=\"Transformer Encoder Block\" class=\"centered-image\">\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V1EBiNZ9XrUS"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_07_encoder.py\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from modules._03_layerNormalisation import LayerNormalization\n",
        "from modules._04_feedForwardNetwork import FeedForwardNetwork\n",
        "from modules._05_multiHeadAttention import MultiHeadAttention\n",
        "from modules._06_residualConnection import ResidualConnection\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardNetwork, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout,features= features) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzj9EdbxXrUS"
      },
      "source": [
        "### 8. Decoder ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3WRG7vqXrUT"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%; /* Adjust the width as needed */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <img src=\"https://raw.githubusercontent.com/hyunwoongko/transformer/master/image/model.png\" alt=\"Transformer Encoder Block\" class=\"centered-image\">\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4LnVVcNnXrUT"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_08_decoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from modules._03_layerNormalisation import LayerNormalization\n",
        "from modules._04_feedForwardNetwork import FeedForwardNetwork\n",
        "from modules._05_multiHeadAttention import MultiHeadAttention\n",
        "from modules._06_residualConnection import ResidualConnection\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardNetwork, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout, features=features) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vApBkGHzXrUT"
      },
      "source": [
        "### 9. Linear / Projection Layer ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYt5T22XrUT"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <style>\n",
        "        .centered-image {\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "            width: 50%; /* Adjust the width as needed */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <img src=\"https://raw.githubusercontent.com/hyunwoongko/transformer/master/image/model.png\" alt=\"Transformer Encoder Block\" class=\"centered-image\">\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4OVgS6O1XrUT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%writefile modules/_09_projectionLayer.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self , d_model:int , vocab_size : int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)\n",
        "        #Dim -1 is the last dimension, Log Softmax is used for numerical stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClCA6ib7XrUT"
      },
      "source": [
        "## **II. Defining & Building the Transformer ->**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zCrinkMeXrUT"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_10_transformer.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from modules._01_inputEmbeddings import InputEmbeddings\n",
        "from modules._02_positionalEncoding import PositionalEncoding\n",
        "from modules._03_layerNormalisation import LayerNormalization\n",
        "from modules._04_feedForwardNetwork import FeedForwardNetwork\n",
        "from modules._05_multiHeadAttention import MultiHeadAttention\n",
        "from modules._06_residualConnection import ResidualConnection\n",
        "from modules._07_encoder import Encoder\n",
        "from modules._08_decoder import Decoder\n",
        "from modules._09_projectionLayer import ProjectionLayer\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder : Encoder,\n",
        "                 decoder : Decoder,\n",
        "                 src_embed : InputEmbeddings,\n",
        "                 tgt_embed : InputEmbeddings,\n",
        "                 src_pos : PositionalEncoding,\n",
        "                 tgt_pos : PositionalEncoding,\n",
        "                 projection_layer : ProjectionLayer):\n",
        "        super().__init__()\n",
        "\n",
        "        #Initialising encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos & projection_layer\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "\n",
        "        return self.encoder(src, src_mask) #Parameter order is same as per forward() of Encoder\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "\n",
        "        return self.decoder(tgt, encoder_output,src_mask, tgt_mask) #Parameter order is same as per forward() of Decoder\n",
        "\n",
        "    def project(self,x):\n",
        "        return self.projection_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a87qnN_XrUT"
      },
      "source": [
        "#### **We have defined all individual parts, Now a function to get all individual parts and build a transformer ->**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vSnSgO2-XrUU"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_11_buildTransformer.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from modules._01_inputEmbeddings import InputEmbeddings\n",
        "from modules._02_positionalEncoding import PositionalEncoding\n",
        "from modules._03_layerNormalisation import LayerNormalization\n",
        "from modules._04_feedForwardNetwork import FeedForwardNetwork\n",
        "from modules._05_multiHeadAttention import MultiHeadAttention\n",
        "from modules._06_residualConnection import ResidualConnection\n",
        "from modules._07_encoder import Encoder , EncoderBlock\n",
        "from modules._08_decoder import Decoder , DecoderBlock\n",
        "from modules._09_projectionLayer import ProjectionLayer\n",
        "from modules._10_transformer import Transformer\n",
        "\n",
        "def build_transformer(src_vocab_size: int ,\n",
        "                      tgt_vocab_size: int ,\n",
        "                      src_seq_len: int ,\n",
        "                      tgt_seq_len: int,\n",
        "                      d_model : int = 512,\n",
        "                      N:int = 6, #According to paper, The number of encoder and decoder blocks is 6.\n",
        "                      h:int = 8, #According to paper, The number of heads is 8.\n",
        "                      dropout:int = 0.1,\n",
        "                      d_ff:int  = 2048 #Dimensions of feed forward network is 2048\n",
        "                      ):\n",
        "\n",
        "    #Creating the embedding layers ->\n",
        "    src_embed = InputEmbeddings(d_model=d_model, vocab_size=src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model=d_model, vocab_size=tgt_vocab_size)\n",
        "\n",
        "    #Creating the positional encoding layers ->\n",
        "    src_pos = PositionalEncoding(d_model=d_model, seq_len=src_seq_len, dropout=dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model=d_model, seq_len=tgt_seq_len, dropout=dropout)\n",
        "\n",
        "    #Creating the encoder blocks ->\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "\n",
        "        encoder_self_attention_block = MultiHeadAttention(d_model=d_model,h=h,dropout=dropout)\n",
        "        encoder_feed_forward_block = FeedForwardNetwork(d_model=d_model,d_ff=d_ff,dropout=dropout)\n",
        "\n",
        "        encoder_block = EncoderBlock(self_attention_block=encoder_self_attention_block, feed_forward_block=encoder_feed_forward_block,dropout=dropout, features=d_model)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    #Creating the decoder blocks ->\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "\n",
        "        decoder_self_attention_block = MultiHeadAttention(d_model=d_model,h=h,dropout=dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttention(d_model=d_model, h=h, dropout=dropout)\n",
        "        decoder_feed_forward_block = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\n",
        "        decoder_block = DecoderBlock(self_attention_block=decoder_self_attention_block, cross_attention_block=decoder_cross_attention_block,feed_forward_block=decoder_feed_forward_block,dropout=dropout,features=d_model)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "\n",
        "    #Creating the encoder and decoder ->\n",
        "    encoder = Encoder(layers=nn.ModuleList(encoder_blocks), features=d_model)\n",
        "    decoder = Decoder(layers=nn.ModuleList(decoder_blocks), features=d_model)\n",
        "\n",
        "    #Creating the projection player ->\n",
        "    projection_layer = ProjectionLayer(d_model=d_model, vocab_size=tgt_vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "    #CREATING THE TRANSFORMER ---->\n",
        "    transformer = Transformer(encoder=encoder,\n",
        "                              decoder=decoder,\n",
        "                              src_embed=src_embed,\n",
        "                              tgt_embed=tgt_embed,\n",
        "                              src_pos=src_pos,\n",
        "                              tgt_pos = tgt_pos,\n",
        "                              projection_layer=projection_layer)\n",
        "\n",
        "    #Initializing the parameters using xavier unuform distribution ->\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim()>1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeRbXpOXXrUU"
      },
      "source": [
        "## **III. Data Preparation & Model Initiation->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5T9IPQxXrUU"
      },
      "source": [
        "- Tokenizer\n",
        "- Dataset\n",
        "- Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV1PUvBeXrUU"
      },
      "source": [
        "### 1 & 2. Tokenizer & Dataset ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNPAW63sXrUV"
      },
      "source": [
        "A **tokenizer** converts a sequence of text into smaller, manageable units called tokens. These tokens can be words, subwords, characters, or even sentences, depending on the specific tokenizer and its configuration.\n",
        "\n",
        "We also have special tokens for the **<<SOS>SOS>** (start of sentence) , **<<SOS>EOS>** (end of sentence) & **Padding**.\n",
        "\n",
        "1. **Text Normalization**: The tokenizer first normalizes the text by converting it to lowercase, removing punctuation, and handling special characters. This step ensures consistency and reduces the complexity of the text.\n",
        "\n",
        "2. **Splitting**: The normalized text is then split into tokens based on predefined rules. For example, a word-level tokenizer might split text at spaces, while a character-level tokenizer would split text into individual characters.\n",
        "\n",
        "3. **Mapping to IDs**: Each token is mapped to a unique identifier (ID) from a vocabulary. This step converts the text into a numerical format that can be processed by machine learning models.\n",
        "\n",
        "4. **Padding**: The tokens are padded to a uniform length. This step ensures that all sequences in the batch have the same length.\n",
        "\n",
        "\n",
        "Consider the sentence: \"Tokenization is essential for NLP.\"\n",
        "\n",
        "- **Normalized Text**: \"tokenization is essential for nlp\"\n",
        "- **Tokens**: [\"tokenization\", \"is\", \"essential\", \"for\", \"nlp\"]\n",
        "- **Token IDs**: [1012, 2003, 3722, 2005, 17953] (IDs are hypothetical)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hoDoZfiHXrUV"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_12_bilingualDataset.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang , seq_len ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.long)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.long)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def causal_mask(self,size):\n",
        "        mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.int)\n",
        "        return mask == 0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Getting the source and target sentences together and then splitting them\n",
        "        src_tgt_pair = self.ds[index]\n",
        "        src_text = src_tgt_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_tgt_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        enc_num_pad_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        dec_num_pad_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        if enc_num_pad_tokens < 0 or dec_num_pad_tokens < 0:\n",
        "            raise ValueError(\"The input sentence is too long!\")\n",
        "\n",
        "        # Adding the SOS, EOS and Padding to encoder input\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.long),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_pad_tokens, dtype=torch.long)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Adding SOS token and padding to decoder input\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "                torch.tensor([self.pad_token] * dec_num_pad_tokens, dtype=torch.long)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Adding EOS token and padding to label (What we expect as output from decoder)\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.long),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_pad_tokens, dtype=torch.long)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Checking the sizes of the tensors\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).long(),  # (1,1,seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).long() & self.causal_mask(decoder_input.size(0)),# (1,1,seq_len)\n",
        "            \"label\": label, #(seq_len)\n",
        "            \"src_text\":src_text,\n",
        "            \"tgt_text\" : tgt_text\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Tob1lKuFXrUV"
      },
      "outputs": [],
      "source": [
        "# %%writefile modules/_13_buildTokenizer_DataLoader_and_Transformer.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from modules._10_transformer import Transformer\n",
        "from modules._11_buildTransformer import build_transformer\n",
        "from modules._12_bilingualDataset import BilingualDataset\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "\n",
        "    if not tokenizer_path.exists():\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_ds(config):\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    ds_raw = load_dataset('cfilt/iitb-english-hindi', 'default', split=\"train[:1%]\")\n",
        "\n",
        "\n",
        "    # Building the tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
        "\n",
        "    # Splitting the training and testing data into 80% & 20% split\n",
        "    train_ds_size = int(0.8 * len(ds_raw))\n",
        "    test_ds_size = len(ds_raw) - train_ds_size\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    train_ds_raw, test_ds_raw = random_split(ds_raw, [train_ds_size, test_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(ds=train_ds_raw, tokenizer_src=tokenizer_src, tokenizer_tgt=tokenizer_tgt, src_lang=config[\"lang_src\"], tgt_lang=config[\"lang_tgt\"], seq_len=config[\"seq_len\"])\n",
        "    test_ds = BilingualDataset(ds=test_ds_raw, tokenizer_src=tokenizer_src, tokenizer_tgt=tokenizer_tgt, src_lang=config[\"lang_src\"], tgt_lang=config[\"lang_tgt\"], seq_len=config[\"seq_len\"])\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    test_dataloader = DataLoader(test_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, test_dataloader , tokenizer_src, tokenizer_tgt\n",
        "\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(src_vocab_size=vocab_src_len, tgt_vocab_size=vocab_tgt_len, src_seq_len=config[\"seq_len\"], tgt_seq_len=config[\"seq_len\"], d_model=config[\"d_model\"])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbleb9wdXrUV"
      },
      "source": [
        "### 3. Config File ->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW4qd3A2XrUW",
        "outputId": "8df62440-700c-4eb3-d063-7177dd6da70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting modules/_14_config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile modules/_14_config.py\n",
        "from pathlib import Path\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 3,\n",
        "        \"lr\": 0.001,\n",
        "        \"seq_len\": 500,\n",
        "        \"d_model\": 256,  #512 in the paper\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"hi\",\n",
        "        \"dataset_config\": \"default\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"transformerModel\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"N\": 2,    #6 in the paper\n",
        "        \"h\": 4,    #8 in the paper\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/transformerModel\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pth\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_-6vIYtXrUW"
      },
      "source": [
        "## **IV. Training and Testing Loops ->**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S33G1hq1XrUW"
      },
      "source": [
        "#### **Training Loop ->**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IiDYyWjXrUW",
        "outputId": "7d77c532-c769-4c07-9a94-467cb159dcc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preloading model weights/transformerModel00.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-783d22f5a632>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_filename)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training from epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 01: 100%|██████████| 1659/1659 [09:14<00:00,  2.99it/s, Loss=2.341]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. Average Loss: 2.8178\n",
            "Model saved: weights/transformerModel01.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 02: 100%|██████████| 1659/1659 [09:15<00:00,  2.98it/s, Loss=1.539]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed. Average Loss: 2.0976\n",
            "Model saved: weights/transformerModel02.pth\n"
          ]
        }
      ],
      "source": [
        "# %%writefile modules/_15_train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from modules._13_buildTokenizer_DataLoader_and_Transformer import get_or_build_tokenizer, get_model, get_ds\n",
        "from modules._14_config import get_config, get_weights_file_path\n",
        "\n",
        "\n",
        "\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = Path(config['model_folder'])\n",
        "    model_files = list(model_folder.glob('*.pth'))\n",
        "    if not model_files:\n",
        "        print(f\"No model files found in {model_folder}\")\n",
        "        return None\n",
        "    return max(model_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "def train_model(config):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, test_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config=config)\n",
        "\n",
        "    model = get_model(config=config, vocab_src_len=tokenizer_src.get_vocab_size(), vocab_tgt_len=tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    preload = config['preload']\n",
        "    if preload == 'latest':\n",
        "        model_filename = latest_weights_file_path(config=config)\n",
        "    elif preload:\n",
        "        model_filename = get_weights_file_path(config, preload)\n",
        "    else:\n",
        "        model_filename = None\n",
        "\n",
        "    if model_filename and Path(model_filename).exists():\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No valid model to preload, starting from scratch')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    print(f\"Starting training from epoch {initial_epoch}\")\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
        "\n",
        "        total_loss =  0\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)\n",
        "            decoder_input = batch[\"decoder_input\"].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            encoder_output = model.encode(src=encoder_input, src_mask=encoder_mask)\n",
        "            decoder_output = model.decode(tgt=decoder_input, encoder_output=encoder_output, src_mask=encoder_mask, tgt_mask=decoder_mask)\n",
        "            project_output = model.projection_layer(decoder_output)\n",
        "\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            loss = loss_fn(project_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_iterator.set_postfix({\"Loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        average_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch} completed. Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"global_step\": global_step,\n",
        "            \"loss\": average_loss\n",
        "        }, model_filename)\n",
        "        print(f\"Model saved: {model_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX9t_rGvXrUX"
      },
      "source": [
        "#### **Validation Loop ->**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yiHYqYdXrUX",
        "outputId": "6618424f-494b-4327-9226-cb068a35906a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading model weights/transformerModel02.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-32-8db9d1fbccad>:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_filename)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: File \"% s\" already exists. Do you want to overwrite it?\n",
            "    TARGET: फ़ाइल से. उपस्थित है को? \n",
            " PREDICTED: फ़ाइल से . उपस्थित है को ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Inspect last focused accessible\n",
            "    TARGET: अंतिम बार फोकस किए गए एक्सेसेबल को जांचें\n",
            " PREDICTED: अंतिम बार फोकस किए गए एक्सेसेबल को जांचें\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Text + Icons\n",
            "    TARGET: Text + Icons\n",
            " PREDICTED: पाठ\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Sensitive\n",
            "    TARGET: संवेदनशील\n",
            " PREDICTED: संवेदनशील\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Name\n",
            "    TARGET: नाम\n",
            " PREDICTED: नाम\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Keep target path:\n",
            "    TARGET: रखें लक्ष्यः\n",
            " PREDICTED: यहां बांटें\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Enable code folding\n",
            "    TARGET: प्रोफाईलिंग पुन सेट करें\n",
            " PREDICTED: सक्षम करें सभी कोड\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Source Directories\n",
            "    TARGET: स्रोत निर्देशिकाएँ\n",
            " PREDICTED: श्रोत दिखाएं\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: king of clubs\n",
            "    TARGET: चिड़ी का बादशाह\n",
            " PREDICTED: चिड़ी का बादशाह\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Show Line Numbers\n",
            "    TARGET: पंक्ति संख्याएं दिखाएं\n",
            " PREDICTED: पंक्ति संख्याएं दिखाएं\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from modules._14_config import get_config, get_weights_file_path\n",
        "from modules._13_buildTokenizer_DataLoader_and_Transformer import get_model, get_ds\n",
        "\n",
        "def greedy_decode(model, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(src, src_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(src).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(src_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, src_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(src).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "import random\n",
        "\n",
        "def run_validation(model, validation_dataloader, tokenizer_src, tokenizer_tgt, max_len, device, num_examples=2):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert dataloader to a list for easy random sampling\n",
        "    validation_data = list(validation_dataloader)\n",
        "\n",
        "    # Randomly sample num_examples from the validation data\n",
        "    sampled_data = random.sample(validation_data, min(num_examples, len(validation_data)))\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in sampled_data:\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)  # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)  # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print('-'*console_width)\n",
        "            print(f\"{'SOURCE: ':>12}{source_text}\")\n",
        "            print(f\"{'TARGET: ':>12}{target_text}\")\n",
        "            print(f\"{'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "        print('-'*console_width)\n",
        "\n",
        "    return source_texts, expected, predicted\n",
        "\n",
        "def validate_model(config):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Get the tokenizers and data loaders\n",
        "    _, validation_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    # Load the pretrained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    if model_filename and Path(model_filename).exists():\n",
        "        print(f'Loading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "    else:\n",
        "        print('No model to load, starting from scratch')\n",
        "\n",
        "    run_validation(model, validation_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, num_examples=10)\n",
        "\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = Path(config['model_folder'])\n",
        "    model_files = list(model_folder.glob('*.pth'))\n",
        "    if not model_files:\n",
        "        return None\n",
        "    return max(model_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = get_config()\n",
        "    validate_model(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Conclusion and Future Work**\n",
        "\n",
        "This project successfully demonstrates the implementation and basic functionality of a Transformer model for machine translation. While the model was trained for only 3 epochs due to computational constraints, it showcases the core concepts and potential of the architecture.\n",
        "\n",
        "### **Training Time and Resource Considerations:**\n",
        "- On a consumer-grade GPU (for example my NVIDIA RTX 3090 4GB), each epoch takes approximately 1.33-1.5 hours & On a higher-end GPU, Like Google Colab's Tesla T4, each epoch takes approximately 10-15 minutes, But that resource is time-limited and thus not practical for this project.\n",
        "- A full training run of 500-600 epochs would require 28-38 days of continuous computation.\n",
        "- Note : This training time is only based on **1%** of the actual dataset + It has lesser number of layers than the actual model. So, Full Data would require 100-200x more time.\n",
        "- State-of-the-art models often train on multiple high-end GPUs or TPUs for months or even years.\n",
        "\n",
        "### **Given these resource requirements, this project focused on:**\n",
        "1. Correct implementation of the Transformer architecture\n",
        "2. Demonstration of the training process & Validation of the model's ability to learn and generate translations.\n",
        "\n",
        "As these requirements are met, this project has reached its conclusion.\n",
        "Future enhancements can be made by training it for longer and improving its performance, Thus achieving a higher accuracy. \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
