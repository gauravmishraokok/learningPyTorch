{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing Transformer by Replicating *Attention Is All You Need***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Input Embeddings -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile modules/01_inputEmbeddings.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model:int, vocab_size:int ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  #Dimensionality -> d_model = 512: You choose to represent each word by a 512-dimensional vector.\n",
    "        self.vocab_size = vocab_size #Number of Tokens \n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Positional Encoding -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "PE(pos, 2i)   &= \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}} \\right) \\\\\n",
    "PE(pos, 2i+1) &= \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}} \\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile modules/02_positionalEncoding.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model:int, seq_len:int, dropout:float ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #Creating a Matrix of shape (seq_len , d_model)\n",
    "        pe = torch.zeros(seq_len,d_model)\n",
    "        \n",
    "        #Creating a position vector of length seq_len\n",
    "        position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1) #[0,1,2,...,n]\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)).float() * (-math.log(10000)/d_model) #This comes from the denominator of the function.\n",
    "        \n",
    "        \n",
    "        #Applying sin to even positions and cos to odd positions\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(dim = 0) # [1,seq_len,d_model]\n",
    "        \n",
    "        #Register Buffer \n",
    "             #--> Is used for saving positonal encoding to model's state_dict as it is not updated during any backward propagation step but is needed for reliability and reusability. \n",
    "        self.register_butter('pe',pe)         \n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        #Input tokens  --> Input tokens  + Position of the respective tokens\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        \n",
    "        #Dropout layer \n",
    "        return self.dropout(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Layer Normalisation -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\large \\hat{x}_j=\\frac{x_j-\\mu_j}{\\sqrt{\\sigma_j^2+\\epsilon}}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile modules/03_layerNormalisation.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    \n",
    "    def __init__(self, eps:float = 10**-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Epsilon is a small value added for numerical stability and also to avoid division by 0\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))    #This is multiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))    #This is added\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim = -1 , keepdim=True) #Usually mean doesnt keep dimension.\n",
    "        std = x.std(dim = -1 , keepdim=True) #Usually std doesnt keep dimension.\n",
    "        \n",
    "        return self.alpha * (x-mean)/torch.sqrt(std + self.eps)   + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feed-Forward Network -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Feed Forward Network (FFN) in the Transformer model is represented as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\large \\operatorname{FFN}(x)=\\max \\left(0, x W_1+b_1\\right) W_2+b_2\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile modules/04_feedForwardNetwork.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model: int , d_ff:int , dropout:float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1=nn.Linear(in_features=d_model,out_features=d_ff) #W1,B1\n",
    "        self.dropout = nn.Dropout(p=dropout) \n",
    "        \n",
    "        self.linear_2 = nn.Linear(d_ff,d_model) #W2,B2\n",
    "        \n",
    "    def forward(self, x): \n",
    "        #(Batch_len , Seq_len , d_model) -> (Batch_len , Seq_len , d_ff) ->(Batch_len , Seq_len , d_model) \n",
    "        \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-Head Attention -> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Multi-Head Attention Workflow -> ](https://i.ibb.co/Y0mbNbH/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model:int,h:int,dropout:float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.h=h\n",
    "        assert d_model%h==0 , 'Embedding dimension must be divisible by number of heads'\n",
    "        \n",
    "        self.d_k = d_model//h  #d_k = d_model/h\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model,d_model) #Wq\n",
    "        self.w_k = nn.Linear(d_model,d_model) #Wk\n",
    "        self.w_v = nn.Linear(d_model,d_model) #Wv\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model,d_model) #Wo as d_v is same as d_k and d_k*h = d_model\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        \n",
    "    @staticmethod    \n",
    "    def attention(query,key,value,mask,dropout:nn.Dropout):\n",
    "        #Calculating the attention score\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2,-1))/math.sqrt(d_k) # @ --> Matrix Multiplication\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0 , -1e9) #-1e9 --> -Infinity. These values later become 0 after softmax\n",
    "            \n",
    "        attention_scores = attention_scores.softmax(dim = -1) #(Batch , h , Seq_len , Seq_len)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores @ value) , attention_scores \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,q,k,v,mask):\n",
    "        \n",
    "        #Getting the Q',K' & V'\n",
    "        \n",
    "        query = self.w_q(q) # (Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
    "        key = self.w_k(k) # (Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
    "        value = self.w_v(v) # (Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
    "        \n",
    "        #Splitting the d_model dimension into h heads\n",
    "        \n",
    "        #Here query.shape[0] -> Batch & query.shape[1] -> Seq_len\n",
    "        #Final shape of query after transpose -> (Batch , h , Seq_len , d_k) \n",
    "        query = query.view(query.shape[0] , query.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0] , key.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        value=key.view(value.shape[0] , value.shape[1],self.h,self.d_k).transpose(1,2) \n",
    "        \n",
    "        x,self.attention_score = MultiHeadAttention.attention(query,key,value,mask,self.dropout)\n",
    "        #Shape of x -> (Batch , h , Seq_len , d_k)\n",
    "        #Shape of attention_score -> (Batch , h , Seq_len , Seq_len)\n",
    "        \n",
    "        x= x.transpose(1,2).contiguous().view(x.shape[0], -1 , self.d_k * self.h) #(Batch , Seq_len , d_model)\n",
    "        \n",
    "        \n",
    "        return self.w_o(x) #(Batch , Seq_len , d_model) -> (Batch , Seq_len , d_model)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
