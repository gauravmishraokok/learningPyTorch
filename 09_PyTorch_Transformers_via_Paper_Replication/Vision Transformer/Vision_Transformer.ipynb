{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision Transformer ->**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/uygarkurt/ViT-PyTorch/main/assets/arc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Defining the Config File / Hyperparameters ->** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_01_config.py\n",
    "import torch\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 26\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIMENSION = 128\n",
    "DROPOUT = 0.1\n",
    "ADAM_WEIGHT_DECAY = 0.01\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 2\n",
    "EMBED_DIM = (PATCH_SIZE**2)*IN_CHANNELS \n",
    "NUM_PATCHES = (IMG_SIZE//PATCH_SIZE)**2 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Patch Embedding ->** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_02_patchEmbedding.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Modules._01_config import * \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels:int = IN_CHANNELS,\n",
    "                 embed_dim:int = EMBED_DIM,\n",
    "                 patch_size:int = PATCH_SIZE,\n",
    "                 num_patches = NUM_PATCHES,\n",
    "                 dropout = DROPOUT):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size), \n",
    "            nn.Flatten(2)\n",
    "        ) #This moves over the image exactly the number of times patches are and in the shape of patches.\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,in_channels,embed_dim)),requires_grad=True) #Shape -> [1,3,48]\n",
    "        \n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)),requires_grad=True) #+1 for CLS Token\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0],-1,-1) #Shape -> [1,3,48] -> [Input_shape,3,48]\n",
    "        \n",
    "        x= self.patcher(x).permute(0,2,1) #Shape Change -> [Input_shape,3,48] -> [Input_shape,48,3]\n",
    "        \n",
    "        x = torch.cat([cls_token, x], dim = 1) \n",
    "\n",
    "        x += self.positional_embeddings \n",
    "        \n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. ViT Implementation ->**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_03_ViT.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Modules._01_config import *\n",
    "from Modules._02_patchEmbedding import PatchEmbedding\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes=NUM_CLASSES,\n",
    "                 patch_size=PATCH_SIZE,\n",
    "                 num_patches=NUM_PATCHES,\n",
    "                 in_channels=IN_CHANNELS,\n",
    "                 embed_dim=EMBED_DIM,\n",
    "                 num_heads=NUM_HEADS,\n",
    "                 hidden_dim=HIDDEN_DIMENSION,\n",
    "                 num_encoders=NUM_ENCODERS,\n",
    "                 activation=ACTIVATION,\n",
    "                 dropout=DROPOUT):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding Block\n",
    "        self.embedding_block = PatchEmbedding(in_channels=in_channels, embed_dim=embed_dim, patch_size=patch_size, num_patches=num_patches, dropout=dropout)\n",
    "        \n",
    "        # Encoder Layer and Encoder Block\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, activation=activation, batch_first=True, norm_first=False)\n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer=self.encoder_layer, num_layers=num_encoders, enable_nested_tensor=False)\n",
    "        \n",
    "        # MLP (Multi Layer Perceptron) Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])  # Taking the CLS Token only\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Dataset Preparation ->**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset -> A-Z Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_04_dataset.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from Modules._01_config import *\n",
    "\n",
    "# Define the path and read the data\n",
    "data_path = Path(\"data\")\n",
    "df = pd.read_csv(data_path / \"A_Z Handwritten Data.csv\").astype(\"float32\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_values = df.iloc[:, 1:].values\n",
    "y_values = df.iloc[:, 0].values\n",
    "\n",
    "# Convert to appropriate numpy types\n",
    "X_values = X_values.astype(np.float32)\n",
    "y_values = y_values.astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X_values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_values, dtype=torch.int64)\n",
    "\n",
    "# Define the transformation\n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToPILImage(),  \n",
    "    transforms.Resize((28, 28)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class HandwrittenDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Reshape image to (1, 28, 28) from (784,)\n",
    "        image = image.reshape(1, 28, 28)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define class names\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] \n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=8888)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = HandwrittenDataset(X_train, y_train, transform=transformation)\n",
    "test_dataset = HandwrittenDataset(X_test, y_test, transform=transformation)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Model Training ->**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intiailized model & Set Weights using Xavier Normal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18623 [00:00<?, ?it/s]c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "100%|██████████| 18623/18623 [03:22<00:00, 91.87it/s] \n",
      "100%|██████████| 4656/4656 [00:23<00:00, 195.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 1: 2.9416\n",
      "Test  Loss EPOCH 1: 2.9361\n",
      "Train Accuracy EPOCH 1: 0.1507\n",
      "Test Accuracy EPOCH 1: 0.1576\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18623/18623 [03:21<00:00, 92.27it/s] \n",
      "100%|██████████| 4656/4656 [00:23<00:00, 195.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 2: 2.9410\n",
      "Test  Loss EPOCH 2: 2.9364\n",
      "Train Accuracy EPOCH 2: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [07:34<11:21, 227.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy EPOCH 2: 0.1576\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18623/18623 [03:24<00:00, 91.04it/s]\n",
      "100%|██████████| 4656/4656 [00:24<00:00, 188.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 3: 2.9408\n",
      "Test  Loss EPOCH 3: 2.9404\n",
      "Train Accuracy EPOCH 3: 0.1509\n",
      "Test Accuracy EPOCH 3: 0.1576\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18623/18623 [03:16<00:00, 95.01it/s] \n",
      "100%|██████████| 4656/4656 [00:24<00:00, 193.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 4: 2.9411\n",
      "Test  Loss EPOCH 4: 2.9404\n",
      "Train Accuracy EPOCH 4: 0.1510\n",
      "Test Accuracy EPOCH 4: 0.1576\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18623/18623 [03:18<00:00, 93.68it/s] \n",
      "100%|██████████| 4656/4656 [00:24<00:00, 187.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 5: 2.9415\n",
      "Test  Loss EPOCH 5: 2.9427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [18:51<00:00, 225.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy EPOCH 5: 0.1508\n",
      "Test Accuracy EPOCH 5: 0.1576\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [18:51<00:00, 226.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# %%writefile Modules/_05_training.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from Modules._01_config import *\n",
    "from Modules._02_patchEmbedding import PatchEmbedding\n",
    "from Modules._03_ViT import ViT\n",
    "from Modules._04_dataset import train_dataloader, test_dataloader, class_names\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "model_folder = Path(\"weights\")\n",
    "model_folder.mkdir(parents=True, exist_ok=True)\n",
    "model_path = model_folder/f\"ViT_{EPOCHS}_epochs.pth\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path).to(device)\n",
    "    print(\"Loaded existing model.\")\n",
    "    \n",
    "else:\n",
    "    visionTransformerModel = ViT(num_classes=NUM_CLASSES, embed_dim=EMBED_DIM, num_heads=NUM_HEADS, num_encoders=NUM_ENCODERS, patch_size=PATCH_SIZE, num_patches=NUM_PATCHES, hidden_dim=HIDDEN_DIMENSION, dropout=DROPOUT).to(device=device)\n",
    "    \n",
    "    for p in visionTransformerModel.parameters():\n",
    "        if p.dim()>1:\n",
    "            p=nn.init.xavier_uniform_(p)\n",
    "\n",
    "    \n",
    "    print(\"Intiailized model & Set Weights using Xavier Normal.\")\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(visionTransformerModel.parameters(), lr=LEARNING_RATE, betas=ADAM_BETAS, weight_decay=ADAM_WEIGHT_DECAY)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "startTime = timer()\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCHS), position=0, leave=True):\n",
    "    \n",
    "    visionTransformerModel.train()\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(tqdm.tqdm(train_dataloader, position=0, leave=True)):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_pred = visionTransformerModel(X)\n",
    "        y_pred_label = y_pred.argmax(dim=1)\n",
    "        \n",
    "        train_labels.extend(y.cpu().detach())\n",
    "        train_preds.extend(y_pred_label.cpu().detach())\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    \n",
    "    visionTransformerModel.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    \n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(tqdm.tqdm(test_dataloader, position=0, leave=True)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = visionTransformerModel(X)\n",
    "            y_pred_label = y_pred.argmax(dim=1)\n",
    "            \n",
    "            val_labels.extend(y.cpu().detach())\n",
    "            val_preds.extend(y_pred_label.cpu().detach())\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "        val_loss = val_loss / len(test_dataloader)\n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"Test  Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy EPOCH {epoch+1}: {sum(1 for x, y in zip(train_preds, train_labels) if x == y) / len(train_labels):.4f}\")\n",
    "    print(f\"Test Accuracy EPOCH {epoch+1}: {sum(1 for x, y in zip(val_preds, val_labels) if x == y) / len(val_labels):.4f}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(visionTransformerModel, model_folder/f\"ViT_{epoch+1}_epochs.pth\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_7880\\604394317.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)\n",
      "  0%|          | 0/4656 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4656/4656 [00:26<00:00, 177.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile Modules/_06_testing_and_plotting.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Modules._01_config import *\n",
    "from Modules._02_patchEmbedding import PatchEmbedding\n",
    "from Modules._03_ViT import ViT\n",
    "from Modules._04_dataset import train_dataloader, test_dataloader, class_names\n",
    "from pathlib import Path\n",
    "\n",
    "labels = []\n",
    "ids = []\n",
    "imgs = []\n",
    "\n",
    "# Load the trained model\n",
    "model_folder = Path(\"weights\")\n",
    "model_path = model_folder / f\"ViT_{EPOCHS-4}_epochs.pth\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch, (X,y) in enumerate(tqdm.tqdm(test_dataloader, position=0, leave=True)):\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "\n",
    "        # ids.extend([int(i)+1 for i in y])\n",
    "        \n",
    "        y_pred_test = model(X)\n",
    "        y_pred_test_label = y_pred_test.argmax(dim=1)\n",
    "        \n",
    "        imgs.extend(X.detach().cpu())\n",
    "        labels.extend([int(i) for i in y_pred_test_label])\n",
    "plt.figure()\n",
    "f, axarr = plt.subplots(3, 4)\n",
    "counter = 0\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if counter < len(imgs):  # Check if the counter is within the range of available images\n",
    "\n",
    "            axarr[i][j].imshow(imgs[counter].squeeze(), cmap=\"gray\")  # Use imgs[counter] to access individual images\n",
    "            axarr[i][j].set_title(f\"Predicted : {class_names[labels[counter]]}\")\n",
    "            axarr[i][j].axis(\"off\")\n",
    "\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model can be trained for more better performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
