{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision Transformer ->**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/uygarkurt/ViT-PyTorch/main/assets/arc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Defining the Config File / Hyperparameters ->** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_01_config.py\n",
    "import torch\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "NUM_CLASSES = 26\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIMENSION = 128\n",
    "DROPOUT = 0.1\n",
    "ADAM_WEIGHT_DECAY = 0.01\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 2\n",
    "EMBED_DIM = (PATCH_SIZE**2)*IN_CHANNELS \n",
    "NUM_PATCHES = (IMG_SIZE//PATCH_SIZE)**2 \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Patch Embedding ->** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_02_patchEmbedding.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Modules._01_config import * \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels:int = IN_CHANNELS,\n",
    "                 embed_dim:int = EMBED_DIM,\n",
    "                 patch_size:int = PATCH_SIZE,\n",
    "                 num_patches = NUM_PATCHES,\n",
    "                 dropout = DROPOUT):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size), \n",
    "            nn.Flatten(2)\n",
    "        ) #This moves over the image exactly the number of times patches are and in the shape of patches.\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,in_channels,embed_dim)),requires_grad=True) #Shape -> [1,3,48]\n",
    "        \n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)),requires_grad=True) #+1 for CLS Token\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0],-1,-1) #Shape -> [1,3,48] -> [Input_shape,3,48]\n",
    "        \n",
    "        x= self.patcher(x).permute(0,2,1) #Shape Change -> [Input_shape,3,48] -> [Input_shape,48,3]\n",
    "        \n",
    "        x = torch.cat([cls_token, x], dim = 1) \n",
    "\n",
    "        x += self.positional_embeddings \n",
    "        \n",
    "        x=self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. ViT Implementation ->**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_03_ViT.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Modules._01_config import *\n",
    "from Modules._02_patchEmbedding import PatchEmbedding\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes = NUM_CLASSES,\n",
    "                 patch_size = PATCH_SIZE,\n",
    "                 num_patches = NUM_PATCHES,\n",
    "                 in_channels = IN_CHANNELS,\n",
    "                 embed_dim = EMBED_DIM,\n",
    "                 num_heads = NUM_HEADS,\n",
    "                 hidden_dim = HIDDEN_DIMENSION,\n",
    "                 num_encoders = NUM_ENCODERS,\n",
    "                 activation = ACTIVATION,\n",
    "                 dropout = DROPOUT):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #Embedding Block\n",
    "        self.embedding_block = PatchEmbedding(in_channels=in_channels,embed_dim=embed_dim,patch_size=patch_size,num_patches=num_patches,dropout=dropout)\n",
    "        \n",
    "        \n",
    "        #Encoder Layer and Encoder Block -> \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim,nhead=num_heads,dropout=dropout,activation=activation,batch_first=True, norm_first=True)\n",
    "        #Batch First = True -> Telling the encoder that in out input shape the batch size comes first.\n",
    "        #Norm First = True -> Layer Normalisation is used before the attention and feed-forward operations.\n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer=self.encoder_layer, num_layers=num_encoders)\n",
    "        \n",
    "        \n",
    "        #MLP (Multi Layer Perceptron) Head -> \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            \n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes)\n",
    "        )        \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.embedding_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) #Taking the CLS Token only\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Dataset Preparation ->**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset -> A-Z Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile Modules/_04_dataset.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from Modules._01_config import *\n",
    "\n",
    "# Define the path and read the data\n",
    "data_path = Path(\"data\")\n",
    "df = pd.read_csv(data_path / \"A_Z Handwritten Data.csv\").astype(\"float32\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_values = df.iloc[:, 1:].values\n",
    "y_values = df.iloc[:, 0].values\n",
    "\n",
    "# Convert to appropriate numpy types\n",
    "X_values = X_values.astype(np.float32)\n",
    "y_values = y_values.astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X_values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_values, dtype=torch.int64)\n",
    "\n",
    "# Define the transformation\n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToPILImage(),  \n",
    "    transforms.Resize((28, 28)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class HandwrittenDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Reshape image to (1, 28, 28) from (784,)\n",
    "        image = image.reshape(1, 28, 28)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define class names\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] \n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = HandwrittenDataset(X_train, y_train, transform=transformation)\n",
    "test_dataset = HandwrittenDataset(X_test, y_test, transform=transformation)\n",
    "\n",
    "BATCH_SIZE=32\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Model Training ->**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized new model, Set weights using Xavier Uniform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9312 [00:00<?, ?it/s]c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "100%|██████████| 9312/9312 [01:52<00:00, 83.02it/s]\n",
      "100%|██████████| 2328/2328 [00:15<00:00, 148.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 1: 2.9384\n",
      "Test  Loss EPOCH 1: 2.9439\n",
      "Train Accuracy EPOCH 1: 0.1524\n",
      "Test Accuracy EPOCH 1: 0.1561\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9312/9312 [01:52<00:00, 83.11it/s]\n",
      "100%|██████████| 2328/2328 [00:15<00:00, 148.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 2: 2.9378\n",
      "Test  Loss EPOCH 2: 2.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:18<06:27, 129.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy EPOCH 2: 0.1523\n",
      "Test Accuracy EPOCH 2: 0.1561\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9312/9312 [01:52<00:00, 82.65it/s]\n",
      "100%|██████████| 2328/2328 [00:15<00:00, 146.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 3: 2.9376\n",
      "Test  Loss EPOCH 3: 2.9411\n",
      "Train Accuracy EPOCH 3: 0.1526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [06:28<04:18, 129.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy EPOCH 3: 0.1561\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9312/9312 [01:48<00:00, 85.54it/s]\n",
      "100%|██████████| 2328/2328 [00:15<00:00, 146.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 4: 2.9372\n",
      "Test  Loss EPOCH 4: 2.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [08:34<02:08, 128.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy EPOCH 4: 0.1523\n",
      "Test Accuracy EPOCH 4: 0.1561\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9312/9312 [01:51<00:00, 83.45it/s]\n",
      "100%|██████████| 2328/2328 [00:14<00:00, 155.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train Loss EPOCH 5: 2.9376\n",
      "Test  Loss EPOCH 5: 2.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:42<00:00, 128.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy EPOCH 5: 0.1522\n",
      "Test Accuracy EPOCH 5: 0.1561\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [10:42<00:00, 128.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# %%writefile Modules/_05_training.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from Modules._01_config import *\n",
    "from Modules._02_patchEmbedding import PatchEmbedding\n",
    "from Modules._03_ViT import ViT\n",
    "from Modules._04_dataset import train_dataloader, test_dataloader, class_names\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model_folder = Path(\"weights\")\n",
    "model_folder.mkdir(parents=True, exist_ok=True)\n",
    "model_path = model_folder/f\"ViT_{EPOCHS}_epochs.pth\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path).to(device)\n",
    "    print(\"Loaded existing model.\")\n",
    "else:\n",
    "    model = ViT().to(device=device)\n",
    "    model.apply(init_weights)\n",
    "    print(\"Initialized new model, Set weights using Xavier Uniform.\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=ADAM_BETAS, weight_decay=ADAM_WEIGHT_DECAY)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "startTime = timer()\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCHS), position=0, leave=True):\n",
    "    \n",
    "    model.train()\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(tqdm.tqdm(train_dataloader, position=0, leave=True)):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        y_pred_label = y_pred.argmax(dim=1)\n",
    "        \n",
    "        train_labels.extend(y.cpu().detach())\n",
    "        train_preds.extend(y_pred_label.cpu().detach())\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    \n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(tqdm.tqdm(test_dataloader, position=0, leave=True)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            y_pred_label = y_pred.argmax(dim=1)\n",
    "            \n",
    "            val_labels.extend(y.cpu().detach())\n",
    "            val_preds.extend(y_pred_label.cpu().detach())\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "        val_loss = val_loss / len(test_dataloader)\n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train Loss EPOCH {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"Test  Loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy EPOCH {epoch+1}: {sum(1 for x, y in zip(train_preds, train_labels) if x == y) / len(train_labels):.4f}\")\n",
    "    print(f\"Test Accuracy EPOCH {epoch+1}: {sum(1 for x, y in zip(val_preds, val_labels) if x == y) / len(val_labels):.4f}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model, model_folder/f\"ViT_{epoch+1}_epochs.pth\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_20412\\1241588267.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)\n",
      "100%|██████████| 2328/2328 [00:18<00:00, 124.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile Modules/_06_testing_and_plotting.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Modules._01_config import *\n",
    "from Modules._02_patchEmbedding import PatchEmbedding\n",
    "from Modules._03_ViT import ViT\n",
    "from Modules._04_dataset import train_dataloader, test_dataloader, class_names\n",
    "from pathlib import Path\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "labels = []\n",
    "ids = []\n",
    "imgs = []\n",
    "\n",
    "# Load the trained model\n",
    "model_folder = Path(\"weights\")\n",
    "model_path = model_folder / f\"ViT_{EPOCHS}_epochs.pth\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch, (X,y) in enumerate(tqdm.tqdm(test_dataloader, position=0, leave=True)):\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "\n",
    "        # ids.extend([int(i)+1 for i in y])\n",
    "        \n",
    "        y_pred_test = model(X)\n",
    "        y_pred_test_label = y_pred_test.argmax(dim=1)\n",
    "        \n",
    "        imgs.extend(X.detach().cpu())\n",
    "        labels.extend([int(i) for i in y_pred_test_label])\n",
    "plt.figure()\n",
    "f, axarr = plt.subplots(3, 4)\n",
    "counter = 0\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        if counter < len(imgs):  # Check if the counter is within the range of available images\n",
    "\n",
    "            axarr[i][j].imshow(imgs[counter].squeeze(), cmap=\"gray\")  # Use imgs[counter] to access individual images\n",
    "            axarr[i][j].set_title(f\"Predicted : {class_names[labels[counter]]}\")\n",
    "            axarr[i][j].axis(\"off\")\n",
    "\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
